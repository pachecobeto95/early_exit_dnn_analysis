{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Early_Exit_DNN_framework_2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1_vss34_uElnI_Y-XHvj8XHzUtIBjXrHG","authorship_tag":"ABX9TyMEZfPWEVbXHInT7VUl5z8B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"PV750LRhiC_L"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import sys, time, math\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import os\n","from itertools import product\n","import pandas as pd\n","import torchvision.transforms as transforms\n","import torchvision\n","import torchvision.models as models\n","from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim import lr_scheduler\n","from torchvision import datasets, transforms\n","from scipy import stats\n","import tarfile\n","from torch.utils.data.sampler import SubsetRandomSampler\n","!pip install pthflops\n","from pthflops import count_ops\n","from torch import Tensor\n","from typing import Callable, Any, Optional, List"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJy28gueipp6","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"error","timestamp":1626786249883,"user_tz":180,"elapsed":2468,"user":{"displayName":"Roberto GonÃ§alves Pacheco","photoUrl":"","userId":"11633798104348778305"}},"outputId":"e538279f-e170-4954-c27c-d5197cc6f149"},"source":["#from torchvision.models.resnet import BasicBlock\n","\n","def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n","  \"\"\"3x3 convolution with padding\"\"\"\n","  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","class BasicBlock(nn.Module):\n","  \"\"\"Basic Block defition.\n","  Basic 3X3 convolution blocks for use on ResNets with layers <= 34.\n","  Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n","  \"\"\"\n","  expansion = 1\n","\n","  def __init__(self, inplanes, planes, stride=1, downsample=None):\n","    super(BasicBlock, self).__init__()\n","    self.conv1 = conv3x3(inplanes, planes, stride)\n","    self.bn1 = nn.BatchNorm2d(planes)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv2 = conv3x3(planes, planes)\n","    self.bn2 = nn.BatchNorm2d(planes)\n","    self.downsample = downsample\n","    self.stride = stride\n","\n","  def forward(self, x):\n","    identity = x\n","\n","    out = self.conv1(x)\n","    out = self.bn1(out)\n","    out = self.relu(out)\n","\n","    out = self.conv2(out)\n","    out = self.bn2(out)\n","\n","    if self.downsample is not None:\n","      identity = self.downsample(x)\n","\n","    out += identity\n","    out = self.relu(out)\n","\n","    return out\n","\n","\n","def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n","    \"\"\"\n","    This function is taken from the original tf repo.\n","    It ensures that all layers have a channel number that is divisible by 8\n","    It can be seen here:\n","    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n","    \"\"\"\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","class ConvBNActivation(nn.Sequential):\n","    def __init__(\n","        self,\n","        in_planes: int,\n","        out_planes: int,\n","        kernel_size: int = 3,\n","        stride: int = 1,\n","        groups: int = 1,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","        activation_layer: Optional[Callable[..., nn.Module]] = None,\n","        dilation: int = 1,\n","    ) -> None:\n","        padding = (kernel_size - 1) // 2 * dilation\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if activation_layer is None:\n","            activation_layer = nn.ReLU6\n","        super().__init__(\n","            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n","                      bias=False),\n","            norm_layer(out_planes),\n","            activation_layer(inplace=True)\n","        )\n","        self.out_channels = out_planes\n","\n","\n","# necessary for backwards compatibility\n","ConvBNReLU = ConvBNActivation\n","\n","\n","class InvertedResidual(nn.Module):\n","    def __init__(\n","        self,\n","        inp: int,\n","        oup: int,\n","        stride: int,\n","        expand_ratio: int,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None\n","    ) -> None:\n","        super(InvertedResidual, self).__init__()\n","        self.stride = stride\n","        assert stride in [1, 2]\n","\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","\n","        hidden_dim = int(round(inp * expand_ratio))\n","        self.use_res_connect = self.stride == 1 and inp == oup\n","\n","        layers: List[nn.Module] = []\n","        if expand_ratio != 1:\n","            # pw\n","            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n","        layers.extend([\n","            # dw\n","            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n","            # pw-linear\n","            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","            norm_layer(oup),\n","        ])\n","        self.conv = nn.Sequential(*layers)\n","        self.out_channels = oup\n","        self._is_cn = stride > 1\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        if self.use_res_connect:\n","            return x + self.conv(x)\n","        else:\n","            return self.conv(x)\n","\n","class EarlyExitBlock(nn.Module):\n","  \"\"\"\n","  This EarlyExitBlock allows the model to terminate early when it is confident for classification.\n","  \"\"\"\n","  def __init__(self, input_shape, n_classes, exit_type, device):\n","    super(EarlyExitBlock, self).__init__()\n","    self.input_shape = input_shape\n","\n","    self.layers = nn.ModuleList()\n","    if (exit_type == 'bnpool'):\n","      self.layers.append(nn.BatchNorm2d(channel))\n","\n","    if (exit_type != 'plain'):\n","      self.layers.append(AvgPool2d(kernel_size))\n","    \n","    #This line defines the data shape that fully-connected layer receives.\n","    current_channel, current_width, current_height = self.get_current_data_shape()\n","\n","    self.layers = self.layers.to(device)\n","\n","    \n","    #This line builds the fully-connected layer\n","    self.classifier = nn.Sequential(nn.Linear(current_channel*current_width*current_height, n_classes))\n","\n","    self.softmax_layer = nn.Softmax(dim=1)\n","  \n","  def get_current_data_shape(self):\n","    _, channel, width, height = self.input_shape\n","    temp_layers = nn.Sequential(*self.layers)\n","\n","    input_tensor = torch.rand(1, channel, width, height)\n","    _, output_channel, output_width, output_height = temp_layers(input_tensor).shape\n","    return output_channel, output_width, output_height\n","        \n","  def forward(self, x):\n","    for layer in self.layers:\n","      x = layer(x)\n","    x = x.view(x.size(0), -1)\n","    output = self.classifier(x)\n","    #confidence = self.softmax_layer()\n","    return output\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","  \"\"\"1x1 convolution\"\"\"\n","  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class Early_Exit_DNN(nn.Module):\n","  def __init__(self, model_name: str, n_classes: int, \n","               pretrained: bool, n_branches: int, input_shape:tuple, \n","               exit_type: str, device, distribution=\"linear\"):\n","    super(Early_Exit_DNN, self).__init__()\n","\n","    \"\"\"\n","    This classes builds an early-exit DNNs architectures\n","    Args:\n","\n","    model_name: model name \n","    n_classes: number of classes in a classification problem, according to the dataset\n","    pretrained: \n","    n_branches: number of branches (early exits) inserted into middle layers\n","    input_shape: shape of the input image\n","    exit_type: type of the exits\n","    distribution: distribution method of the early exit blocks.\n","    device: indicates if the model will processed in the cpu or in gpu\n","    \n","    Note: the term \"backbone model\" refers to a regular DNN model, considering no early exits.\n","\n","    \"\"\"\n","    self.model_name = model_name\n","    self.n_classes = n_classes\n","    self.pretrained = pretrained\n","    self.n_branches = n_branches\n","    self.input_shape = input_shape\n","    self.exit_type = exit_type\n","    self.distribution = distribution\n","    self.device = device\n","    self.channel, self.width, self.height = input_shape\n","\n","\n","    build_early_exit_dnn = self.select_dnn_architecture_model()\n","\n","    build_early_exit_dnn()\n","\n","  def select_dnn_architecture_model(self):\n","    \"\"\"\n","    This method selects the backbone to insert the early exits.\n","    \"\"\"\n","\n","    architecture_dnn_model_dict = {\"alexnet\": self.early_exit_alexnet,\n","                                   \"mobilenet\": self.early_exit_mobilenet,\n","                                   \"resnet18\": self.early_exit_resnet18,\n","                                   \"resnet34\": self.early_exit_resnet34}\n","\n","    return architecture_dnn_model_dict.get(self.model_name, self.invalid_model)\n","\n","  def select_distribution_method(self):\n","    \"\"\"\n","    This method selects the distribution method to insert early exits into the middle layers.\n","    \"\"\"\n","    distribution_method_dict = {\"linear\":self.linear_distribution,\n","                                \"pareto\":self.paretto_distribution,\n","                                \"fibonacci\":self.fibo_distribution}\n","    return distribution_method_dict.get(self.distribution, self.invalid_distribution)\n","    \n","  def linear_distribution(self, i):\n","    \"\"\"\n","    This method defines the Flops to insert an early exits, according to a linear distribution.\n","    \"\"\"\n","    flop_margin = 1.0 / (self.n_branches+1)\n","    return self.total_flops * flop_margin * (i+1)\n","\n","  def paretto_distribution(self, i):\n","    \"\"\"\n","    This method defines the Flops to insert an early exits, according to a pareto distribution.\n","    \"\"\"\n","    return self.total_flops * (1 - (0.8**(i+1)))\n","\n","  def fibo_distribution(self, i):\n","    \"\"\"\n","    This method defines the Flops to insert an early exits, according to a fibonacci distribution.\n","    \"\"\"\n","    gold_rate = 1.61803398875\n","    return total_flops * (gold_rate**(i - self.num_ee))\n","\n","  def verifies_nr_exits(self, backbone_model):\n","    \"\"\"\n","    This method verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    \"\"\"\n","    \n","    total_layers = len(list(backbone_model.children()))\n","    if (self.n_branches >= total_layers):\n","      raise Exception(\"The number of early exits is greater than number of layers in the DNN backbone model.\")\n","\n","  def countFlops(self, model):\n","    \"\"\"\n","    This method counts the numper of Flops in a given full DNN model or intermediate DNN model.\n","    \"\"\"\n","    input = torch.rand(1, self.channel, self.width, self.height)\n","    flops, all_data = count_ops(model, input, print_readable=False, verbose=False)\n","    return flops\n","\n","  def where_insert_early_exits(self):\n","    \"\"\"\n","    This method defines where insert the early exits, according to the dsitribution method selected.\n","    Args:\n","\n","    total_flops: Flops of the backbone (full) DNN model.\n","    \"\"\"\n","    threshold_flop_list = []\n","    distribution_method = self.select_distribution_method()\n","\n","    for i in range(self.n_branches):\n","      threshold_flop_list.append(distribution_method(i))\n","\n","    return threshold_flop_list\n","\n","  def invalid_model(self):\n","    raise Exception(\"This DNN model has not implemented yet.\")\n","  def invalid_distribution(self):\n","    raise Exception(\"This early-exit distribution has not implemented yet.\")\n","\n","  def is_suitable_for_exit(self):\n","    \"\"\"\n","    This method answers the following question. Is the position to place an early exit?\n","    \"\"\"\n","    intermediate_model = nn.Sequential(*(list(self.stages)+list(self.layers)))\n","    current_flop = self.countFlops(intermediate_model)\n","    return self.stage_id < self.n_branches and current_flop >= self.threshold_flop_list[self.stage_id]\n","\n","  def add_exit_block(self):\n","    \"\"\"\n","    This method adds an early exit in the suitable position.\n","    \"\"\"\n","    input_tensor = torch.rand(1, self.channel, self.width, self.height)\n","    self.stages.append(nn.Sequential(*self.layers))\n","    feature_shape = nn.Sequential(*self.stages)(input_tensor).shape\n","    self.exits.append(EarlyExitBlock(feature_shape, self.n_classes, self.exit_type, self.device))\n","    self.layers = nn.ModuleList()\n","    self.stage_id += 1    \n","\n","  def set_device(self):\n","    \"\"\"\n","    This method sets the device that will run the DNN model.\n","    \"\"\"\n","\n","    self.stages.to(self.device)\n","    self.exits.to(self.device)\n","    self.layers.to(self.device)\n","    self.classifier.to(self.device)\n","\n","\n","  def early_exit_alexnet(self):\n","    \"\"\"\n","    This method inserts early exits into a Alexnet model\n","    \"\"\"\n","\n","    self.stages = nn.ModuleList()\n","    self.exits = nn.ModuleList()\n","    self.layers = nn.ModuleList()\n","    self.cost = []\n","    self.stage_id = 0\n","\n","    # Loads the backbone model. In other words, Alexnet architecture provided by Pytorch.\n","    backbone_model = models.alexnet(self.pretrained)\n","\n","    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    self.verifies_nr_exit_alexnet(backbone_model.features)\n","    \n","    # This obtains the flops total of the backbone model\n","    self.total_flops = self.countFlops(backbone_model)\n","\n","    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n","    self.threshold_flop_list = self.where_insert_early_exits()\n","\n","    for layer in backbone_model.features:\n","      self.layers.append(layer)\n","      if (isinstance(layer, nn.ReLU)) and (self.is_suitable_for_exit()):\n","        self.add_exit_block()\n","\n","    \n","    \n","    self.layers.append(nn.AdaptiveAvgPool2d(output_size=(6, 6)))\n","    self.stages.append(nn.Sequential(*self.layers))\n","\n","    \n","    self.classifier = backbone_model.classifier\n","    self.classifier[6] = nn.Linear(in_features=4096, out_features=self.n_classes, bias=True)\n","    self.softmax = nn.Softmax(dim=1)\n","    self.set_device()\n","\n","  def verifies_nr_exit_alexnet(self, backbone_model):\n","    \"\"\"\n","    This method verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    In AlexNet, we consider a convolutional block composed by: Convolutional layer, ReLU and he Max-pooling layer.\n","    Hence, we consider that it makes no sense to insert side branches between these layers or only after the convolutional layer.\n","    \"\"\"\n","\n","    count_relu_layer = 0\n","    for layer in backbone_model:\n","      if (isinstance(layer, nn.ReLU)):\n","        count_relu_layer += 1\n","\n","    if (count_relu_layer > self.n_branches):\n","      raise Exception(\"The number of early exits is greater than number of layers in the DNN backbone model.\")\n","\n","  def early_exit_resnet18(self):\n","    \"\"\"\n","    This method inserts early exits into a Resnet18 model\n","    \"\"\"\n","\n","    self.stages = nn.ModuleList()\n","    self.exits = nn.ModuleList()\n","    self.layers = nn.ModuleList()\n","    self.cost = []\n","    self.stage_id = 0\n","\n","    self.inplanes = 64\n","\n","    n_blocks = 4\n","\n","    backbone_model = models.resnet18(self.pretrained)\n","\n","    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    self.verifies_nr_exits(backbone_model)\n","\n","    # This obtains the flops total of the backbone model\n","    self.total_flops = self.countFlops(backbone_model)\n","\n","    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n","    self.threshold_flop_list = self.where_insert_early_exits()\n","\n","    building_first_layer = [\"conv1\", \"bn1\", \"relu\", \"maxpool\"]\n","    for layer in building_first_layer:\n","      self.layers.append(getattr(backbone_model, layer))\n","\n","    if (self.is_suitable_for_exit()):\n","      self.add_exit_block()\n","\n","    for i in range(1, n_blocks+1):\n","      \n","      block_layer = getattr(backbone_model, \"layer%s\"%(i))\n","\n","      for l in block_layer:\n","        self.layers.append(l)\n","\n","        if (self.is_suitable_for_exit()):\n","          self.add_exit_block()\n","    \n","    self.layers.append(nn.AdaptiveAvgPool2d(1))\n","    self.classifier = nn.Sequential(nn.Linear(512, self.n_classes))\n","    self.stages.append(nn.Sequential(*self.layers))\n","    self.softmax = nn.Softmax(dim=1)\n","    self.set_device()\n","\n","  def early_exit_resnet34(self):\n","    return True\n","  \n","\n","  def early_exit_mobilenet(self):\n","    \"\"\"\n","    This method inserts early exits into a Mobilenet V2 model\n","    \"\"\"\n","\n","    self.stages = nn.ModuleList()\n","    self.exits = nn.ModuleList()\n","    self.layers = nn.ModuleList()\n","    self.cost = []\n","    self.stage_id = 0\n","\n","    last_channel = 1280\n","    \n","    # Loads the backbone model. In other words, Mobilenet architecture provided by Pytorch.\n","    backbone_model = models.mobilenet_v2(self.pretrained)\n","\n","    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    self.verifies_nr_exits(backbone_model.features)\n","    \n","    # This obtains the flops total of the backbone model\n","    self.total_flops = self.countFlops(backbone_model)\n","\n","    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n","    self.threshold_flop_list = self.where_insert_early_exits()\n","\n","    for i, layer in enumerate(backbone_model.features.children()):\n","      \n","      self.layers.append(layer)    \n","      if (self.is_suitable_for_exit()):\n","        self.add_exit_block()\n","\n","    self.layers.append(nn.AdaptiveAvgPool2d(1))    \n","\n","    self.classifier = nn.Sequential(\n","        nn.Dropout(0.2),\n","        nn.Linear(last_channel, self.n_classes),)\n","\n","    self.set_device()\n","\n","  def forwardTrain(self, x):\n","    \"\"\"\n","    This method is used to train the early-exit DNN model\n","    \"\"\"\n","    \n","    output_list, conf_list, class_list  = [], [], []\n","\n","    for i, exitBlock in enumerate(self.exits):\n","\n","      x = self.stages[i](x)\n","      output_branch = exitBlock(x)\n","      output_list.append(output_branch)\n","\n","      #Confidence is the maximum probability of belongs one of the predefined classes and inference_class is the argmax\n","      conf, infered_class = torch.max(self.softmax(output_branch), 1)\n","      conf_list.append(conf)\n","      class_list.append(infered_class)\n","\n","    x = self.stages[-1](x)\n","\n","    x = torch.flatten(x, 1)\n","\n","    output = self.classifier(x)\n","    infered_conf, infered_class = torch.max(self.softmax(output), 1)\n","    output_list.append(output)\n","    conf_list.append(infered_conf)\n","    class_list.append(infered_class)\n","    return output_list, conf_list, class_list\n","\n","  def forwardEval(self, x, p_tar):\n","    \"\"\"\n","    This method is used to train the early-exit DNN model\n","    \"\"\"\n","    output_list, conf_list, class_list  = [], [], []\n","    for i, exitBlock in enumerate(self.exits):\n","      x = self.stages[i](x)\n","      output_branch = exitBlock(x)\n","      conf, infered_class = torch.max(self.softmax(output_branch), 1)\n","\n","      # Note that if confidence value is greater than a p_tar value, we terminate the dnn inference and returns the output\n","      if (conf.item() >= p_tar):\n","        return output_branch, conf, infered_class, i+1\n","\n","      else:\n","        output_list.append(output_branch)\n","        conf_list.append(conf)\n","        class_list.append(infered_class)\n","\n","    x = self.stages[-1](x)\n","    \n","    x = torch.flatten(x, 1)\n","\n","    output = self.classifier(x)\n","    conf, infered_class = torch.max(self.softmax(output), 1)\n","    \n","    # Note that if confidence value is greater than a p_tar value, we terminate the dnn inference and returns the output\n","    # This also happens in the last exit\n","    if (conf.item() >= p_tar):\n","      return output, conf, infered_class, self.n_branches \n","    else:\n","\n","      # If any exit can reach the p_tar value, the output is give by the more confidence output.\n","      # If evaluation, it returns max(output), max(conf) and the number of the early exit.\n","\n","      conf_list.append(conf)\n","      class_list.append(infered_class)\n","      output_list.append(output)\n","      max_conf = np.argmax(conf_list)\n","      return output_list[max_conf], conf_list[max_conf], class_list[max_conf], self.n_branches\n","\n","  def forward(self, x, p_tar=0.5, training=True):\n","    \"\"\"\n","    This implementation supposes that, during training, this method can receive a batch containing multiple images.\n","    However, during evaluation, this method supposes an only image.\n","    \"\"\"\n","    if (training):\n","      return self.forwardTrain(x)\n","    else:\n","      return self.forwardEval(x, p_tar)\n","\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f5d9e6d7b332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from torchvision.models.resnet import BasicBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mconv3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_planes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;34m\"\"\"3x3 convolution with padding\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"iDq3aS7pBVQ6"},"source":["model_name = \"mobilenet\"\n","n_classes = 258\n","pretrained = True\n","n_branches = 5\n","input_shape = (3, 224, 224)\n","exit_type = [\"plain\"]\n","distribution = \"linear\"\n","\n","ee_model = Early_Exit_DNN(model_name, n_classes, pretrained, n_branches, input_shape, exit_type, distribution)\n"],"execution_count":null,"outputs":[]}]}