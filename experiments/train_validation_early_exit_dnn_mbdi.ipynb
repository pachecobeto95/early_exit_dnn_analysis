{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_validation_early_exit_dnn_mbdi.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1MS_yQYyIYpz9_CTTxFwFrrko9DfACcJ_","authorship_tag":"ABX9TyM2bLIfKYvzPclLEtqlpMwQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eiet4T_SCIEA","executionInfo":{"status":"ok","timestamp":1626747105661,"user_tz":180,"elapsed":9117,"user":{"displayName":"Roberto Gonçalves Pacheco","photoUrl":"","userId":"11633798104348778305"}},"outputId":"8e586700-7d9b-4eea-dfa1-fa452623ff21"},"source":["import torchvision.transforms as transforms\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision.utils import save_image\n","import os, cv2, sys, time, math, os\n","from torchvision import transforms, utils, datasets\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, WeightedRandomSampler\n","from torch.utils.data import random_split\n","from PIL import Image\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from itertools import product\n","import pandas as pd\n","import torchvision.models as models\n","from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim import lr_scheduler\n","from torchvision import datasets, transforms\n","!pip install pthflops\n","from pthflops import count_ops\n","!git clone https://github.com/eksuas/eenets.pytorch.git\n","!mv ./eenets.pytorch ./eenets\n","from pthflops import count_ops\n","from eenets.flops_counter import get_model_complexity_info\n","from torch import Tensor\n","from typing import Callable, Any, Optional, List"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pthflops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pthflops) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pthflops) (3.7.4.3)\n","fatal: destination path 'eenets.pytorch' already exists and is not an empty directory.\n","mv: cannot move './eenets.pytorch' to './eenets/eenets.pytorch': Directory not empty\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IkGm1epsC6xA","executionInfo":{"status":"ok","timestamp":1626747106235,"user_tz":180,"elapsed":576,"user":{"displayName":"Roberto Gonçalves Pacheco","photoUrl":"","userId":"11633798104348778305"}}},"source":["class LoadDataset():\n","  def __init__(self, input_dim, batch_size_train, batch_size_test, save_idx, seed=42):\n","    self.input_dim = input_dim\n","    self.batch_size_train = batch_size_train\n","    self.batch_size_test = batch_size_test\n","    self.seed = seed\n","    self.save_idx = save_idx\n","\n","    #To normalize the input images data.\n","    mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","    std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","    # Note that we apply data augmentation in the training dataset.\n","    self.transformations_train = transforms.Compose([transforms.Resize((input_dim, input_dim)),\n","                                                     transforms.RandomChoice([\n","                                                                              transforms.ColorJitter(brightness=(0.80, 1.20)),\n","                                                                              transforms.RandomGrayscale(p = 0.25)]),\n","                                                     transforms.RandomHorizontalFlip(p = 0.25),\n","                                                     transforms.RandomRotation(25),\n","                                                     transforms.ToTensor(), \n","                                                     transforms.Normalize(mean = mean, std = std),\n","                                                     ])\n","\n","    # Note that we do not apply data augmentation in the test dataset.\n","    self.transformations_test = transforms.Compose([\n","                                                     transforms.Resize(input_dim), \n","                                                     transforms.CenterCrop(input_dim), \n","                                                     transforms.ToTensor(), \n","                                                     transforms.Normalize(mean = mean, std = std),\n","                                                     ])\n","\n","  def cifar_10(self, root_path, split_ratio):\n","    # This method loads Cifar-10 dataset. \n","    \n","    # saves the seed\n","    torch.manual_seed(self.seed)\n","\n","    # This downloads the training and test CIFAR-10 datasets and also applies transformation  in the data.\n","    train_set = datasets.CIFAR10(root=root_path, train=True, download=True, transform=self.transformations_train)\n","    test_set = datasets.CIFAR10(root=root_path, train=False, download=True, transform=self.transformations_test)\n","\n","    classes_list = train_set.classes\n","\n","    # This line defines the size of validation dataset.\n","    val_size = int(split_ratio*len(train_set))\n","\n","    # This line defines the size of training dataset.\n","    train_size = int(len(train_set) - val_size)\n","\n","    #This line splits the training dataset into train and validation, according split ratio provided as input.\n","    train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n","\n","    #This block creates data loaders for training, validation and test datasets.\n","    train_loader = DataLoader(train_dataset, self.batch_size_train, shuffle=True, num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, self.batch_size_test, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_set, self.batch_size_test, num_workers=4, pin_memory=True)\n","\n","    return train_loader, val_loader, test_loader\n","\n","  def cifar_100(self, root_path, split_ratio):\n","    # This method loads Cifar-100 dataset\n","    root = \"cifar_100\"\n","    torch.manual_seed(self.seed)\n","\n","    # This downloads the training and test Cifar-100 datasets and also applies transformation  in the data.\n","    train_set = datasets.CIFAR100(root=root_path, train=True, download=True, transform=self.transformations_train)\n","    test_set = datasets.CIFAR100(root=root_path, train=False, download=True, transform=self.transformations_train)\n","\n","    classes_list = train_set.classes\n","\n","    # This line defines the size of validation dataset.\n","    val_size = int(split_ratio*len(train_set))\n","\n","    # This line defines the size of training dataset.\n","    train_size = int(len(train_set) - val_size)\n","\n","    #This line splits the training dataset into train and validation, according split ratio provided as input.\n","    train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n","\n","    #This block creates data loaders for training, validation and test datasets.\n","    train_loader = DataLoader(train_dataset, self.batch_size_train, shuffle=True, num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, self.batch_size_test, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_set, self.batch_size_test, num_workers=4, pin_memory=True)\n","\n","    return train_loader, val_loader, test_loader\n","  \n","  def get_indices(self, dataset, split_ratio):\n","    nr_samples = len(dataset)\n","    indices = list(range(nr_samples))\n","    \n","    train_size = nr_samples - int(np.floor(split_ratio * nr_samples))\n","\n","    np.random.shuffle(indices)\n","\n","    train_idx, test_idx = indices[:train_size], indices[train_size:]\n","\n","    return train_idx, test_idx\n","\n","  def caltech_256(self, root_path, split_ratio, savePath_idx):\n","    # This method loads the Caltech-256 dataset.\n","\n","    root = \"caltech_256\"\n","    torch.manual_seed(self.seed)\n","    np.random.seed(seed=None)\n","\n","    # This block receives the dataset path and applies the transformation data. \n","    train_set = datasets.ImageFolder(root_path, transform=self.transformations_train)\n","\n","    val_set = datasets.ImageFolder(root_path, transform=self.transformations_test)\n","    test_set = datasets.ImageFolder(root_path, transform=self.transformations_test)\n","\n","    # This line get the indices of the samples which belong to the training dataset and test dataset. \n","    train_idx, test_idx = self.get_indices(train_set, split_ratio)\n","\n","    # This line mounts the training and test dataset, selecting the samples according indices. \n","    train_data = torch.utils.data.Subset(train_set, indices=train_idx)\n","    test_data = torch.utils.data.Subset(test_set, indices=test_idx)\n","\n","    # This line gets the indices to split the train dataset into training dataset and validation dataset.\n","    train_idx, val_idx = self.get_indices(train_data, split_ratio)\n","\n","    if (self.save_idx):\n","      np.save(os.path.join(savePath_idx, \"validation_idx_caltech256.npy\"), val_idx)\n","      np.save(os.path.join(savePath_idx, \"test_idx_caltech256.npy\"), test_idx)\n","\n","\n","    train_data = torch.utils.data.Subset(train_set, indices=train_idx)\n","    val_data = torch.utils.data.Subset(val_set, indices=val_idx)\n","\n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=self.batch_size_train, shuffle=True, num_workers=4)\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=self.batch_size_test, num_workers=4)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size=self.batch_size_test, num_workers=4)\n","\n","    return train_loader, val_loader, test_loader \n","\n","  def getDataset(self, root_path, dataset_name, split_ratio, savePath_idx):\n","    self.dataset_name = dataset_name\n","    def func_not_found():\n","      print(\"No dataset %s is found\"%(self.dataset_name))\n","\n","    func_name = getattr(self, self.dataset_name, func_not_found)\n","    train_loader, val_loader, test_loader = func_name(root_path, split_ratio, savePath_idx)\n","    return train_loader, val_loader, test_loader\n","\n","#input_dim = 224\n","#batch_size_train = 128\n","#batch_size_test = 1\n","#split_ratio = 0.1\n","#dataset_root_path = \"./drive/MyDrive/undistorted_datasets/Caltech256/256_ObjectCategories/\"\n","#dataset = LoadDataset(input_dim, batch_size_train, batch_size_test)\n","#dataset.getDataset(\"cifar10\", \"cifar_10\", split_ratio)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXSN1FCKEY2O","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"error","timestamp":1626790697038,"user_tz":180,"elapsed":2487,"user":{"displayName":"Roberto Gonçalves Pacheco","photoUrl":"","userId":"11633798104348778305"}},"outputId":"5e5c3361-6ffd-4707-ccb0-a5767bfc0964"},"source":["#from torchvision.models.resnet import BasicBlock\n","\n","def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n","  \"\"\"3x3 convolution with padding\"\"\"\n","  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","class BasicBlock(nn.Module):\n","  \"\"\"Basic Block defition.\n","  Basic 3X3 convolution blocks for use on ResNets with layers <= 34.\n","  Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n","  \"\"\"\n","  expansion = 1\n","\n","  def __init__(self, inplanes, planes, stride=1, downsample=None):\n","    super(BasicBlock, self).__init__()\n","    self.conv1 = conv3x3(inplanes, planes, stride)\n","    self.bn1 = nn.BatchNorm2d(planes)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv2 = conv3x3(planes, planes)\n","    self.bn2 = nn.BatchNorm2d(planes)\n","    self.downsample = downsample\n","    self.stride = stride\n","\n","  def forward(self, x):\n","    identity = x\n","\n","    out = self.conv1(x)\n","    out = self.bn1(out)\n","    out = self.relu(out)\n","\n","    out = self.conv2(out)\n","    out = self.bn2(out)\n","\n","    if self.downsample is not None:\n","      identity = self.downsample(x)\n","\n","    out += identity\n","    out = self.relu(out)\n","\n","    return out\n","\n","\n","def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n","    \"\"\"\n","    This function is taken from the original tf repo.\n","    It ensures that all layers have a channel number that is divisible by 8\n","    It can be seen here:\n","    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n","    \"\"\"\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","class ConvBNActivation(nn.Sequential):\n","    def __init__(\n","        self,\n","        in_planes: int,\n","        out_planes: int,\n","        kernel_size: int = 3,\n","        stride: int = 1,\n","        groups: int = 1,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","        activation_layer: Optional[Callable[..., nn.Module]] = None,\n","        dilation: int = 1,\n","    ) -> None:\n","        padding = (kernel_size - 1) // 2 * dilation\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if activation_layer is None:\n","            activation_layer = nn.ReLU6\n","        super().__init__(\n","            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n","                      bias=False),\n","            norm_layer(out_planes),\n","            activation_layer(inplace=True)\n","        )\n","        self.out_channels = out_planes\n","\n","\n","# necessary for backwards compatibility\n","ConvBNReLU = ConvBNActivation\n","\n","\n","class InvertedResidual(nn.Module):\n","    def __init__(\n","        self,\n","        inp: int,\n","        oup: int,\n","        stride: int,\n","        expand_ratio: int,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None\n","    ) -> None:\n","        super(InvertedResidual, self).__init__()\n","        self.stride = stride\n","        assert stride in [1, 2]\n","\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","\n","        hidden_dim = int(round(inp * expand_ratio))\n","        self.use_res_connect = self.stride == 1 and inp == oup\n","\n","        layers: List[nn.Module] = []\n","        if expand_ratio != 1:\n","            # pw\n","            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n","        layers.extend([\n","            # dw\n","            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n","            # pw-linear\n","            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","            norm_layer(oup),\n","        ])\n","        self.conv = nn.Sequential(*layers)\n","        self.out_channels = oup\n","        self._is_cn = stride > 1\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        if self.use_res_connect:\n","            return x + self.conv(x)\n","        else:\n","            return self.conv(x)\n","\n","class EarlyExitBlock(nn.Module):\n","  \"\"\"\n","  This EarlyExitBlock allows the model to terminate early when it is confident for classification.\n","  \"\"\"\n","  def __init__(self, input_shape, n_classes, exit_type):\n","    super(EarlyExitBlock, self).__init__()\n","    self.input_shape = input_shape\n","\n","    self.layers = []\n","    if (exit_type == 'bnpool'):\n","      self.layers.append(nn.BatchNorm2d(channel))\n","\n","\n","    if (exit_type != 'plain'):\n","      self.layers.append(AvgPool2d(kernel_size))\n","    \n","    #This line defines the data shape that fully-connected layer receives.\n","    current_channel, current_width, current_height = self.get_current_data_shape()\n","\n","    #This line builds the fully-connected layer\n","    self.classifier = nn.Sequential(nn.Linear(current_channel*current_width*current_height, n_classes))\n","\n","    self.softmax_layer = nn.Softmax(dim=1)\n","  \n","  def get_current_data_shape(self):\n","    _, channel, width, height = self.input_shape\n","    temp_layers = nn.Sequential(*self.layers)\n","\n","    input_tensor = torch.rand(1, channel, width, height)\n","    _, output_channel, output_width, output_height = temp_layers(input_tensor).shape\n","    return output_channel, output_width, output_height\n","        \n","  def forward(self, x):\n","\n","    for layer in self.layers:\n","      x = layer(x)\n","    x = x.view(x.size(0), -1)\n","    output = self.classifier(x)\n","    confidence = self.softmax_layer(output)\n","    return output\n","    #return output, confidence\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","  \"\"\"1x1 convolution\"\"\"\n","  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class Early_Exit_DNN(nn.Module):\n","  def __init__(self, model_name: str, n_classes: int, \n","               pretrained: bool, n_branches: int, input_shape:tuple, \n","               exit_type: str, device, distribution=\"linear\"):\n","    super(Early_Exit_DNN, self).__init__()\n","\n","    \"\"\"\n","    This classes builds an early-exit DNNs architectures\n","    Args:\n","\n","    model_name: model name \n","    n_classes: number of classes in a classification problem, according to the dataset\n","    pretrained: \n","    n_branches: number of branches (early exits) inserted into middle layers\n","    input_shape: shape of the input image\n","    exit_type: type of the exits\n","    distribution: distribution method of the early exit blocks.\n","    \"\"\"\n","    self.model_name = model_name\n","    self.n_classes = n_classes\n","    self.pretrained = pretrained\n","    self.n_branches = n_branches\n","    self.input_shape = input_shape\n","    self.exit_type = exit_type\n","    self.distribution = distribution\n","    self.device = device\n","    self.channel, self.width, self.height = input_shape\n","\n","\n","    build_early_exit_dnn = self.select_dnn_architecture_model()\n","\n","    build_early_exit_dnn()\n","\n","  def select_dnn_architecture_model(self):\n","    architecture_dnn_model_dict = {\"alexnet\": self.early_exit_alexnet,\n","                                   \"mobilenet\": self.early_exit_mobilenet,\n","                                   \"resnet18\": self.early_exit_resnet18,\n","                                   \"resnet34\": self.early_exit_resnet34}\n","\n","    return architecture_dnn_model_dict.get(self.model_name, self.invalid_model)\n","\n","  def select_distribution_method(self):\n","    \"\"\"\n","    This method selects the distribution method to insert early exits into the middle layers.\n","    \"\"\"\n","    distribution_method_dict = {\"linear\":self.linear_distribution,\n","                                \"pareto\":self.paretto_distribution,\n","                                \"fibonacci\":self.fibo_distribution}\n","    return distribution_method_dict.get(self.distribution, self.invalid_distribution)\n","    \n","  def linear_distribution(self, i):\n","    \"\"\"\n","    This method defines the Flops to insert an early exits, according to a linear distribution.\n","    \"\"\"\n","    flop_margin = 1.0 / (self.n_branches+1)\n","    return self.total_flops * flop_margin * (i+1)\n","\n","  def paretto_distribution(self, i):\n","    \"\"\"\n","    This method defines the Flops to insert an early exits, according to a pareto distribution.\n","    \"\"\"\n","    return self.total_flops * (1 - (0.8**(i+1)))\n","\n","  def fibo_distribution(self, i):\n","    \"\"\"\n","    This method defines the Flops to insert an early exits, according to a fibonacci distribution.\n","    \"\"\"\n","    gold_rate = 1.61803398875\n","    return total_flops * (gold_rate**(i - self.num_ee))\n","\n","  def verifies_nr_exits(self, backbone_model):\n","    \"\"\"\n","    This method verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    \"\"\"\n","    \n","    total_layers = len(list(backbone_model.children()))\n","    if (self.n_branches >= total_layers):\n","      raise Exception(\"The number of early exits is greater than number of layers in the DNN backbone model.\")\n","\n","  def countFlops(self, model):\n","    \"\"\"\n","    This method counts the numper of Flops in a given full DNN model or intermediate DNN model.\n","    \"\"\"\n","    input = torch.rand(1, self.channel, self.width, self.height)\n","    flops, all_data = count_ops(model, input, print_readable=False, verbose=False)\n","    return flops\n","\n","  def where_insert_early_exits(self):\n","    \"\"\"\n","    This method defines where insert the early exits, according to the dsitribution method selected.\n","    Args:\n","\n","    total_flops: Flops of the backbone (full) DNN model.\n","    \"\"\"\n","    threshold_flop_list = []\n","    distribution_method = self.select_distribution_method()\n","\n","    for i in range(self.n_branches):\n","      threshold_flop_list.append(distribution_method(i))\n","\n","    return threshold_flop_list\n","\n","  def invalid_model(self):\n","    raise Exception(\"This DNN model has not implemented yet.\")\n","  def invalid_distribution(self):\n","    raise Exception(\"This early-exit distribution has not implemented yet.\")\n","\n","  def is_suitable_for_exit(self):\n","    \"\"\"\n","    This method answers the following question. Is the position to place an early exit?\n","    \"\"\"\n","    intermediate_model = nn.Sequential(*(list(self.stages)+list(self.layers)))\n","    current_flop = self.countFlops(intermediate_model)\n","    return self.stage_id < self.n_branches and current_flop >= self.threshold_flop_list[self.stage_id]\n","\n","  def set_device(self):\n","    self.stages.to(self.device)\n","    self.exits.to(self.device)\n","    self.layers.to(self.device)\n","    self.classifier.to(self.device)\n","\n","  def add_exit_block(self):\n","    \"\"\"\n","    This method adds an early exit in the suitable position.\n","    \"\"\"\n","    input_tensor = torch.rand(1, self.channel, self.width, self.height)\n","    self.stages.append(nn.Sequential(*self.layers))\n","    feature_shape = nn.Sequential(*self.stages)(input_tensor).shape\n","    self.exits.append(EarlyExitBlock(feature_shape, self.n_classes, self.exit_type))\n","    self.layers = nn.ModuleList()\n","    self.stage_id += 1    \n","\n","  def early_exit_alexnet(self):\n","\n","    self.stages = nn.ModuleList()\n","    self.exits = nn.ModuleList()\n","    self.layers = nn.ModuleList()\n","    self.cost = []\n","    self.stage_id = 0\n","\n","    # Loads the backbone model. In other words, Alexnet architecture provided by Pytorch.\n","    backbone_model = models.alexnet(self.pretrained)\n","\n","    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    self.verifies_nr_exit_alexnet(backbone_model.features)\n","    \n","    # This obtains the flops total of the backbone model\n","    self.total_flops = self.countFlops(backbone_model)\n","\n","    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n","    self.threshold_flop_list = self.where_insert_early_exits()\n","\n","    for layer in backbone_model.features:\n","      self.layers.append(layer)\n","      if (isinstance(layer, nn.ReLU)) and (self.is_suitable_for_exit()):\n","        self.add_exit_block()\n","\n","    self.layers.append(nn.AdaptiveAvgPool2d(output_size=(6, 6)))\n","    self.stages.append(nn.Sequential(*self.layers))\n","\n","    \n","    self.classifier = backbone_model.classifier\n","    self.classifier[6] = nn.Linear(in_features=4096, out_features=self.n_classes, bias=True)\n","    self.softmax = nn.Softmax(dim=1)\n","    self.set_device()\n","\n","  def verifies_nr_exit_alexnet(self, backbone_model):\n","    \"\"\"\n","    This method verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    In AlexNet, we consider a convolutional block composed by: Convolutional layer, ReLU and he Max-pooling layer.\n","    Hence, we consider that it makes no sense to insert side branches between these layers or only after the convolutional layer.\n","    \"\"\"\n","\n","    count_relu_layer = 0\n","    for layer in backbone_model:\n","      if (isinstance(layer, nn.ReLU)):\n","        count_relu_layer += 1\n","\n","    if (count_relu_layer > self.n_branches):\n","      raise Exception(\"The number of early exits is greater than number of layers in the DNN backbone model.\")\n","\n","  def early_exit_resnet18(self):\n","\n","    self.stages = nn.ModuleList()\n","    self.exits = nn.ModuleList()\n","    self.layers = nn.ModuleList()\n","    self.cost = []\n","    self.stage_id = 0\n","\n","    self.inplanes = 64\n","\n","    n_blocks = 4\n","\n","    backbone_model = models.resnet18(self.pretrained)\n","\n","    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    self.verifies_nr_exits(backbone_model)\n","\n","    # This obtains the flops total of the backbone model\n","    self.total_flops = self.countFlops(backbone_model)\n","\n","    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n","    self.threshold_flop_list = self.where_insert_early_exits()\n","\n","    building_first_layer = [\"conv1\", \"bn1\", \"relu\", \"maxpool\"]\n","    for layer in building_first_layer:\n","      self.layers.append(getattr(backbone_model, layer))\n","\n","    if (self.is_suitable_for_exit()):\n","      self.add_exit_block()\n","\n","    for i in range(1, n_blocks+1):\n","      \n","      block_layer = getattr(backbone_model, \"layer%s\"%(i))\n","\n","      for l in block_layer:\n","        self.layers.append(l)\n","\n","        if (self.is_suitable_for_exit()):\n","          self.add_exit_block()\n","    \n","    self.layers.append(nn.AdaptiveAvgPool2d(1))\n","    self.classifier = nn.Sequential(nn.Linear(512, self.n_classes))\n","    self.stages.append(nn.Sequential(*self.layers))\n","    self.softmax = nn.Softmax(dim=1)\n","    self.set_device()    \n","\n","\n","\n","  def early_exit_resnet34(self):\n","    return True\n","  \n","\n","  def early_exit_mobilenet(self):\n","    self.stages = nn.ModuleList()\n","    self.exits = nn.ModuleList()\n","    self.layers = nn.ModuleList()\n","    self.cost = []\n","    self.stage_id = 0\n","\n","    last_channel = 1280\n","    \n","    # Loads the backbone model. In other words, Mobilenet architecture provided by Pytorch.\n","    backbone_model = models.mobilenet_v2(self.pretrained)\n","\n","    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n","    self.verifies_nr_exits(backbone_model.features)\n","    \n","    # This obtains the flops total of the backbone model\n","    self.total_flops = self.countFlops(backbone_model)\n","\n","    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n","    self.threshold_flop_list = self.where_insert_early_exits()\n","\n","    for i, layer in enumerate(backbone_model.features.children()):\n","      \n","      self.layers.append(layer)    \n","      if (self.is_suitable_for_exit()):\n","        self.add_exit_block()\n","\n","    self.layers.append(nn.AdaptiveAvgPool2d(1))    \n","\n","    self.stages.append(nn.Sequential(*self.layers))\n","\n","    self.classifier = nn.Sequential(\n","        nn.Dropout(0.2),\n","        nn.Linear(last_channel, self.n_classes),)\n","\n","    self.softmax = nn.Softmax(dim=1)\n","    self.set_device()\n","\n","  def forwardTrain(self, x):\n","    output_list, conf_list, class_list  = [], [], []\n","\n","    for i, exitBlock in enumerate(self.exits):\n","\n","      x = self.stages[i](x)\n","      output_branch = exitBlock(x)\n","      output_list.append(output_branch)\n","      conf, infered_class = torch.max(self.softmax(output_branch), 1)\n","      conf_list.append(conf)\n","      class_list.append(infered_class)\n","\n","    x = self.stages[-1](x)\n","\n","    x = torch.flatten(x, 1)\n","\n","    output = self.classifier(x)\n","    infered_conf, infered_class = torch.max(self.softmax(output), 1)\n","    output_list.append(output)\n","    conf_list.append(infered_conf)\n","    class_list.append(infered_class)\n","    return output_list, conf_list, class_list\n","\n","  def forwardEval(self, x, p_tar):\n","    output_list, conf_list, class_list  = [], [], []\n","    for i, exitBlock in enumerate(self.exits):\n","      x = self.stages[i](x)\n","      output_branch = exitBlock(x)\n","      conf, infered_class = torch.max(self.softmax(output_branch), 1)\n","\n","      if (conf.item() >= p_tar):\n","        return output_branch, conf, infered_class, i+1\n","\n","      else:\n","        output_list.append(output_branch)\n","        conf_list.append(conf)\n","        class_list.append(infered_class)\n","\n","    x = self.stages[-1](x)\n","    \n","    x = torch.flatten(x, 1)\n","\n","    output = self.classifier(x)\n","    conf, infered_class = torch.max(self.softmax(output), 1)\n","    \n","    if (conf.item() >= p_tar):\n","      return output, conf, infered_class, self.n_branches \n","    else:\n","      conf_list.append(conf)\n","      class_list.append(infered_class)\n","      output_list.append(output)\n","      max_conf = np.argmax(conf_list)\n","      return output_list[max_conf], conf_list[max_conf], class_list[max_conf], self.n_branches\n","  \n","  def forward(self, x, p_tar=0.5, training=True):\n","    if (training):\n","      return self.forwardTrain(x)\n","    else:\n","      return self.forwardEval(x, p_tar)\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3dd0ba9ce6c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from torchvision.models.resnet import BasicBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mconv3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_planes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;34m\"\"\"3x3 convolution with padding\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"vLybDY8v7Hu8","executionInfo":{"status":"ok","timestamp":1626748192905,"user_tz":180,"elapsed":353,"user":{"displayName":"Roberto Gonçalves Pacheco","photoUrl":"","userId":"11633798104348778305"}}},"source":["\n","\n","\n","def trainBranches(model, train_loader, optimizer, criterion, n_branches, epoch, device, loss_weights):\n","\n","  running_loss = []\n","  train_acc_dict = {i: [] for i in range(1, (n_branches+1)+1)}\n","\n","  metric = Metric(\"precision\")\n","  for i, (data, target) in enumerate(train_loader, 1):\n","    print(\"Batch: %s/%s\"%(i, len(train_loader)))\n","    data, target = data.to(device), target.long().to(device)\n","\n","    output_list, conf_list, class_list = model(data)\n","\n","    optimizer.zero_grad()\n","\n","    optimizer.zero_grad()\n","    loss = 0\n","    for j, (output, inf_class, weight) in enumerate(zip(output_list, class_list, loss_weights), 1):\n","      loss += weight*criterion(output, target)\n","      train_acc_dict[j].append(100*inf_class.eq(target.view_as(inf_class)).sum().item()/target.size(0))\n","\n","\n","    running_loss.append(float(loss.item()))\n","    loss.backward()\n","    optimizer.step()\n","    \n","\n","    # clear variables\n","    del data, target, output_list, conf_list, class_list\n","    torch.cuda.empty_cache()\n","\n","  loss = round(np.average(running_loss), 4)\n","  print(\"Epoch: %s\"%(epoch))\n","  print(\"Train Loss: %s\"%(loss))\n","\n","  result_dict = {\"epoch\":epoch, \"train_loss\": loss}\n","  for key, value in train_acc_dict.items():\n","    result_dict.update({\"train_acc_branch_%s\"%(key): round(np.average(train_acc_dict[key]), 4)})    \n","    print(\"Train Acc Branch %s: %s\"%(key, result_dict[\"train_acc_branch_%s\"%(key)]))\n","  \n","  return result_dict\n","\n","def evalBranches(model, val_loader, criterion, n_branches, epoch, device):\n","  running_loss = []\n","  val_acc_dict = {i: [] for i in range(1, (n_branches+1)+1)}\n","  model.eval()\n","\n","  for i, (data, target) in enumerate(val_loader, 1):\n","    print(\"Batch: %s / %s\"%(i, len(val_loader)))\n","    data, target = data.to(device), target.long().to(device)\n","\n","    output_list, conf_list, class_list = model(data)\n","\n","    loss = 0\n","    for j, (output, inf_class, weight) in enumerate(zip(output_list, class_list, loss_weights), 1):\n","      loss += weight*criterion(output, target)\n","      val_acc_dict[j].append(100*inf_class.eq(target.view_as(inf_class)).sum().item()/target.size(0))\n","\n","\n","    running_loss.append(float(loss.item()))    \n","\n","    # clear variables\n","    del data, target, output_list, conf_list, class_list\n","    torch.cuda.empty_cache()\n","\n","  loss = round(np.average(running_loss), 4)\n","  print(\"Epoch: %s\"%(epoch))\n","  print(\"Val Loss: %s\"%(loss))\n","\n","  result_dict = {\"epoch\":epoch, \"val_loss\": loss}\n","  for key, value in val_acc_dict.items():\n","    result_dict.update({\"val_acc_branch_%s\"%(key): round(np.average(val_acc_dict[key]), 4)})    \n","    print(\"Val Acc Branch %s: %s\"%(key, result_dict[\"val_acc_branch_%s\"%(key)]))\n","  \n","  return result_dict\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eBQEYRlEfP8","executionInfo":{"status":"ok","timestamp":1626747115215,"user_tz":180,"elapsed":6011,"user":{"displayName":"Roberto Gonçalves Pacheco","photoUrl":"","userId":"11633798104348778305"}},"outputId":"703e6f22-0d90-49ec-c661-cb570a763c7a"},"source":["model_name = \"mobilenet\"\n","dataset_name = \"caltech\"\n","model_id = 1\n","\n","\n","\n","root_dir = \"./drive/MyDrive/early_exit_test\"\n","dataset_path = \"./drive/MyDrive/undistorted_datasets/Caltech256/256_ObjectCategories\"\n","\n","model_save_path = os.path.join(root_dir, \"%s_%s_%s.pth\"%(model_name, dataset_name, model_id))\n","history_save_path = os.path.join(root_dir, \"history_%s_%s_%s.pth\"%(model_name, dataset_name, model_id))\n","savePath_idx_dataset = os.path.join(root_dir, \"save_idx_%s_%s\"%(model_name, dataset_name))\n","if (not os.path.exists(savePath_idx_dataset)):\n","  os.makedirs(savePath_idx_dataset)\n","\n","img_dim = 300\n","batch_size_train, batch_size_test = 32, 1\n","split_ratio = 0.1\n","save_idx = True\n","\n","dataset = LoadDataset(img_dim, batch_size_train, batch_size_test, save_idx)\n","train_loader, val_loader, test_loader = dataset.caltech_256(dataset_path, split_ratio, savePath_idx_dataset)\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VbcCIxUR89-","executionInfo":{"status":"ok","timestamp":1626747118111,"user_tz":180,"elapsed":2899,"user":{"displayName":"Roberto Gonçalves Pacheco","photoUrl":"","userId":"11633798104348778305"}},"outputId":"b2c2d085-4f27-4a2e-d4a4-8e1b35e728e5"},"source":["n_classes = 258\n","pretrained = True\n","n_branches = 5\n","img_shape = (3, 300, 300)\n","exit_type = None\n","lr = [1.5e-4, 0.001]\n","weight_decay = 0.00005\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_name = \"mobilenet\"\n","exit_type = [\"plain\"]\n","metric = \"accuracy\"\n","\n","early_exit_dnn_model = Early_Exit_DNN(model_name, n_classes, pretrained, n_branches, img_shape, exit_type, device)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam([{'params': early_exit_dnn_model.stages.parameters(), 'lr': lr[0]},\n","                       {'params': early_exit_dnn_model.exits.parameters(), 'lr': lr[1]},\n","                       {'params': early_exit_dnn_model.classifier.parameters(), 'lr': lr[1]}], weight_decay=weight_decay)\n","\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=0, last_epoch=-1)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Input size: (1, 3, 300, 300)\n","589,580,360 FLOPs or approx. 0.59 GFLOPs\n","Input size: (1, 3, 300, 300)\n","20,880,000 FLOPs or approx. 0.02 GFLOPs\n","Input size: (1, 3, 300, 300)\n","41,040,000 FLOPs or approx. 0.04 GFLOPs\n","Input size: (1, 3, 300, 300)\n","99,090,000 FLOPs or approx. 0.10 GFLOPs\n","Input size: (1, 3, 300, 300)\n","149,040,000 FLOPs or approx. 0.15 GFLOPs\n","Input size: (1, 3, 300, 300)\n","179,133,664 FLOPs or approx. 0.18 GFLOPs\n","Input size: (1, 3, 300, 300)\n","200,666,592 FLOPs or approx. 0.20 GFLOPs\n","Input size: (1, 3, 300, 300)\n","222,199,520 FLOPs or approx. 0.22 GFLOPs\n","Input size: (1, 3, 300, 300)\n","236,870,560 FLOPs or approx. 0.24 GFLOPs\n","Input size: (1, 3, 300, 300)\n","256,508,960 FLOPs or approx. 0.26 GFLOPs\n","Input size: (1, 3, 300, 300)\n","276,147,360 FLOPs or approx. 0.28 GFLOPs\n","Input size: (1, 3, 300, 300)\n","295,785,760 FLOPs or approx. 0.30 GFLOPs\n","Input size: (1, 3, 300, 300)\n","319,837,024 FLOPs or approx. 0.32 GFLOPs\n","Input size: (1, 3, 300, 300)\n","362,602,528 FLOPs or approx. 0.36 GFLOPs\n","Input size: (1, 3, 300, 300)\n","405,368,032 FLOPs or approx. 0.41 GFLOPs\n","Input size: (1, 3, 300, 300)\n","435,627,360 FLOPs or approx. 0.44 GFLOPs\n","Input size: (1, 3, 300, 300)\n","467,659,360 FLOPs or approx. 0.47 GFLOPs\n","Input size: (1, 3, 300, 300)\n","499,691,360 FLOPs or approx. 0.50 GFLOPs\n","Input size: (1, 3, 300, 300)\n","547,083,360 FLOPs or approx. 0.55 GFLOPs\n","Input size: (1, 3, 300, 300)\n","588,299,360 FLOPs or approx. 0.59 GFLOPs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"Z0UGQsDKxYuk","executionInfo":{"status":"error","timestamp":1626748138938,"user_tz":180,"elapsed":39364,"user":{"displayName":"Roberto Gonçalves Pacheco","photoUrl":"","userId":"11633798104348778305"}},"outputId":"4656d0cf-3209-40e3-9bfd-66adfac4e03f"},"source":["loss_weights = [.3, .5, .7, 1]\n","\n","epoch = 0\n","count = 0\n","best_val_loss = np.inf\n","patience = 10\n","\n","df = pd.DataFrame()\n","while 1:\n","  epoch+=1\n","  print(\"Epoch: %s\"%(epoch))\n","  result = {}\n","\n","  result.update(trainBranches(early_exit_dnn_model, train_loader, optimizer, criterion, n_branches, epoch, device, loss_weights))\n","  scheduler.step()\n","  result.update(evalBranches(early_exit_dnn_model, val_loader, criterion, n_branches, epoch, device))\n","\n","  df = df.append(pd.Series(result), ignore_index=True)\n","  df.to_csv(history_save_path)\n","\n","  if (result[\"val_loss\"] < best_val_loss):\n","    best_val_loss = result[\"val_loss\"]\n","    count = 0\n","    save_dict = {\"model_state_dict\": branchynet.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(),\n","                 \"epoch\": epoch, \"val_loss\": result[\"val_loss\"]}\n","    \n","    for i in range(1, n_branches+1+1):\n","      save_dict.update({\"val_acc_branch_%s\"%(i): result[\"val_acc_branch_%s\"%(i)]})\n","\n","    torch.save(save_dict, model_save_path)\n","\n","  else:\n","    count += 1\n","    if (count > patience):\n","      print(\"Stop! Patience is finished\")\n","      break\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch: 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["Batch: 1/775\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-d09d51757631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBranches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_exit_dnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_branches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalBranches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_exit_dnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_branches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-76a5d4063347>\u001b[0m in \u001b[0;36mtrainBranches\u001b[0;34m(model, train_loader, optimizer, criterion, n_branches, epoch, device, loss_weights)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculatePerformanceMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minf_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0;31m#train_acc_dict[j].append(100*inf_class.eq(target.view_as(inf_class)).sum().item()/target.size(0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-76a5d4063347>\u001b[0m in \u001b[0;36mcalculatePerformanceMetric\u001b[0;34m(self, output, target)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfunc_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-76a5d4063347>\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1670\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 152\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Singleton array 146 cannot be considered a valid collection."]}]}]}