{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"load_dataset_Early_Exit_dnn_analysis.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1u5VKRrEg-ZQ4MTuY_shnJlXAullpw1AR","authorship_tag":"ABX9TyNVKjAPTou5SMSqEl7vnuWZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"aYJU9taZQa5o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626384379503,"user_tz":180,"elapsed":2506,"user":{"displayName":"Roberto Gon√ßalves Pacheco","photoUrl":"","userId":"11633798104348778305"}},"outputId":"f40b958d-164a-45af-da25-78f8b027a338"},"source":["import torchvision.transforms as transforms\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision.utils import save_image\n","import os, cv2\n","from torchvision import transforms, utils, datasets\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, WeightedRandomSampler\n","from torch.utils.data import random_split\n","from PIL import Image\n","import torch\n","import numpy as np\n","\n","\n","class LoadDataset():\n","  def __init__(self, input_dim, batch_size_train, batch_size_test, seed=42):\n","    self.input_dim = input_dim\n","    self.batch_size_train = batch_size_train\n","    self.batch_size_test = batch_size_test\n","    self.seed = seed\n","\n","    #To normalize the input images data.\n","    mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","    std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","    # Note that we apply data augmentation in the training dataset.\n","    self.transformations_train = transforms.Compose([transforms.Resize((input_dim, input_dim)),\n","                                                     transforms.RandomChoice([\n","                                                                              transforms.ColorJitter(brightness=(0.80, 1.20)),\n","                                                                              transforms.RandomGrayscale(p = 0.25)]),\n","                                                     transforms.RandomHorizontalFlip(p = 0.25),\n","                                                     transforms.RandomRotation(25),\n","                                                     transforms.ToTensor(), \n","                                                     transforms.Normalize(mean = mean, std = std),\n","                                                     ])\n","\n","    # Note that we do not apply data augmentation in the test dataset.\n","    self.transformations_test = transforms.Compose([\n","                                                     transforms.Resize(input_dim), \n","                                                     transforms.CenterCrop(input_dim), \n","                                                     transforms.ToTensor(), \n","                                                     transforms.Normalize(mean = mean, std = std),\n","                                                     ])\n","\n","  def cifar_10(self, root_path, split_ratio):\n","    # This method loads Cifar-10 dataset. \n","    \n","    # saves the seed\n","    torch.manual_seed(self.seed)\n","\n","    # This downloads the training and test CIFAR-10 datasets and also applies transformation  in the data.\n","    train_set = datasets.CIFAR10(root=root_path, train=True, download=True, transform=self.transformations_train)\n","    test_set = datasets.CIFAR10(root=root_path, train=False, download=True, transform=self.transformations_test)\n","\n","    classes_list = train_set.classes\n","\n","    # This line defines the size of validation dataset.\n","    val_size = int(split_ratio*len(train_set))\n","\n","    # This line defines the size of training dataset.\n","    train_size = int(len(train_set) - val_size)\n","\n","    #This line splits the training dataset into train and validation, according split ratio provided as input.\n","    train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n","\n","    #This block creates data loaders for training, validation and test datasets.\n","    train_loader = DataLoader(train_dataset, self.batch_size_train, shuffle=True, num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, self.batch_size_test, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_set, self.batch_size_test, num_workers=4, pin_memory=True)\n","\n","    return train_loader, val_loader, test_loader\n","\n","  def cifar_100(self, root_path, split_ratio):\n","    # This method loads Cifar-100 dataset\n","    root = \"cifar_100\"\n","    torch.manual_seed(self.seed)\n","\n","    # This downloads the training and test Cifar-100 datasets and also applies transformation  in the data.\n","    train_set = datasets.CIFAR100(root=root_path, train=True, download=True, transform=self.transformations_train)\n","    test_set = datasets.CIFAR100(root=root_path, train=False, download=True, transform=self.transformations_train)\n","\n","    classes_list = train_set.classes\n","\n","    # This line defines the size of validation dataset.\n","    val_size = int(split_ratio*len(train_set))\n","\n","    # This line defines the size of training dataset.\n","    train_size = int(len(train_set) - val_size)\n","\n","    #This line splits the training dataset into train and validation, according split ratio provided as input.\n","    train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n","\n","    #This block creates data loaders for training, validation and test datasets.\n","    train_loader = DataLoader(train_dataset, self.batch_size_train, shuffle=True, num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, self.batch_size_test, num_workers=4, pin_memory=True)\n","    test_loader = DataLoader(test_set, self.batch_size_test, num_workers=4, pin_memory=True)\n","\n","    return train_loader, val_loader, test_loader\n","  \n","  def get_indices(self, dataset, split_ratio):\n","    nr_samples = len(dataset)\n","    indices = list(range(nr_samples))\n","    \n","    train_size = nr_samples - int(np.floor(split_ratio * nr_samples))\n","\n","    np.random.shuffle(indices)\n","\n","    train_idx, test_idx = indices[:train_size], indices[train_size:]\n","\n","    return train_idx, test_idx\n","\n","  def caltech_256(self, root_path, split_ratio):\n","    # This method loads the Caltech-256 dataset.\n","\n","    root = \"caltech_256\"\n","    torch.manual_seed(self.seed)\n","    np.random.seed(seed=None)\n","\n","    # This block receives the dataset path and applies the transformation data. \n","    train_set = datasets.ImageFolder(root_path, transform=self.transformations_train)\n","    print(len(train_set))\n","    val_set = datasets.ImageFolder(root_path, transform=self.transformations_test)\n","    test_set = datasets.ImageFolder(root_path, transform=self.transformations_test)\n","\n","    # This line get the indices of the samples which belong to the training dataset and test dataset. \n","    train_idx, test_idx = self.get_indices(train_set, split_ratio)\n","\n","    # This line mounts the training and test dataset, selecting the samples according indices. \n","    train_data = torch.utils.data.Subset(train_set, indices=train_idx)\n","    test_data = torch.utils.data.Subset(test_set, indices=test_idx)\n","\n","    # This line gets the indices to split the train dataset into training dataset and validation dataset.\n","    train_idx, val_idx = self.get_indices(train_data, split_ratio)\n","\n","    train_data = torch.utils.data.Subset(train_set, indices=train_idx)\n","    val_data = torch.utils.data.Subset(val_set, indices=val_idx)\n","\n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=self.batch_size_train, shuffle=True, num_workers=4)\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=self.batch_size_test, num_workers=4)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size=self.batch_size_test, num_workers=4)\n","\n","    return train_loader, val_loader, test_loader \n","\n","  def getDataset(self, root_path, dataset_name, split_ratio):\n","    self.dataset_name = dataset_name\n","    def func_not_found():\n","      print(\"No dataset %s is found\"%(self.dataset_name))\n","\n","    func_name = getattr(self, self.dataset_name, func_not_found)\n","    train_loader, val_loader, test_loader = func_name(root_path, split_ratio)\n","    return train_loader, val_loader, test_loader\n","\n","input_dim = 224\n","batch_size_train = 128\n","batch_size_test = 1\n","split_ratio = 0.1\n","dataset_root_path = \"./drive/MyDrive/undistorted_datasets/Caltech256/256_ObjectCategories/\"\n","dataset = LoadDataset(input_dim, batch_size_train, batch_size_test)\n","dataset.getDataset(\"cifar10\", \"cifar_10\", split_ratio)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x7f3acbeaed50>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7f3a66d97ed0>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7f3a66d97690>)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"puJDMc-DEXpI"},"source":[""],"execution_count":null,"outputs":[]}]}